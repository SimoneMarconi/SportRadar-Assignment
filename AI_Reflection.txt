The parts of the project I worked on without the use of AI are:
- Design of the database schema and queries
- Design of frontend and backend interactions
- Design of the Flask implementation
- Design of frontend components

The parts generated entirely by AI are the dummy data entries inserted into the database. The prompt for this was:
I am going to provide you with the tables of a database written in Python 3 for SQLite3. I want you to create at least 20 matches with at least 15 teams and 10 locations. The data should follow these rules:
- Include the following sports: football, basketball, baseball
- Add a short description for every team
- You donâ€™t need to add descriptions to every match
- Make sure that the number of sold tickets for a match does not exceed the number of available seats for the location
- Not every team has a stable training location, so you can leave some fields as NULL

With this prompt, I was able to successfully generate valid data to test and use in my database.
The list of constraints was the most important part of the prompt, as it guided the output I wanted and prevented the LLM from generating hallucinations or incoherent data.

I also used AI to help me write the CSS styling for the HTML elements.
All the generated CSS was later reviewed by me, since the AI initially produced a lot of unnecessary rules. My approach was to first write all the required HTML elements for a component and then use that skeleton code in the prompt for the AI to generate the styling.
Additionally, after defining the style used for the main page, I included those CSS rules in every subsequent prompt to give better context about the desired appearance of my UI.

One of the most important decisions I made during this project concerned when and how to connect to the database to retrieve data. I decided to minimize database calls to enhance frontend performance, as all the main filters are applied client-side and the database is queried mainly to fetch the complete dataset (depending on the requested page).
This approach works well as long as the dataset is not too large. If the database grows and query times become too slow, possible improvements could include trimming old data from queries, implementing multiple pages to have progressive queries or implementing a caching system to limit database calls.

If I had more time and resources, I would add a feature to purchase tickets directly from the site, as well as an integration with Google Maps to better display match locations and provide directions to reach them (currently, I only provide coordinates).

The main AI tools I used are Cursor (for code generation, like the css files), ChatGPT and Claude AI (for questions when programming).